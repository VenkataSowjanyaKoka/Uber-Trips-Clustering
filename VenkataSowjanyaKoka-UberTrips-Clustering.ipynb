{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for structured data we use Spark SQL, SparkSession acts a pipeline between data and sql statements\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksession is like a class and we need to create an instance of a class to utilize\n",
    "spark = SparkSession.builder.appName(\"K-Means_Clustering_Data_Processing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file data\n",
    "Uber_DF = spark.read.csv(\"/Users/sowjanyakoka/Desktop/Spring2020/MachineLearning/UberApril14.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1. What is the shape of the data contained in the UberApril14.CSV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (564516, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Answer:\n",
    "#==========================================================================\n",
    "#Seeing the shape of the dataset\n",
    "print(\"Shape:\", (Uber_DF.count(), len(Uber_DF.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Lon: double (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the schema\n",
    "Uber_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2. How many Uber trips were recorded for each company (by base code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TLC base company codes affiliated with Uber pickup : 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Answer:\n",
    "#=========================================================================\n",
    "#Checking how many distinct companies are there in our data\n",
    "Distinct_TLC_Codes = Uber_DF.select('Base').distinct().count()\n",
    "print(\"Number of TLC base company codes affiliated with Uber pickup :\", Distinct_TLC_Codes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips recorded for each company (by base code) :\n",
      "+------+------+\n",
      "|  Base| count|\n",
      "+------+------+\n",
      "|B02682|227808|\n",
      "|B02598|183263|\n",
      "|B02617|108001|\n",
      "|B02512| 35536|\n",
      "|B02764|  9908|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trips recorded for each company (by base code) :\")\n",
    "Uber_DF.groupBy('Base').count().orderBy('count',ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2. What can you say about the distribution trips among companies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#B02682(Schmecken) is the first company that has highest(227808) distribution of trips\n",
    "#B02598(Hinter LLC) is the second company with high(183263) distribution of trips\n",
    "#B02617(Weiter) is the third company with high(108001) distribution of trips\n",
    "#B02512(Unter LLC) company has less(35536) distribution of trips\n",
    "#B02764(Danach-NY) company has least(9908) distribution of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+-------------------+------+\n",
      "|summary|Date/Time       |Lat                 |Lon                |Base  |\n",
      "+-------+----------------+--------------------+-------------------+------+\n",
      "|count  |564516          |564516              |564516             |564516|\n",
      "|mean   |null            |40.74000520746969   |-73.97681683902599 |null  |\n",
      "|stddev |null            |0.036083205020168534|0.05042582837278683|null  |\n",
      "|min    |4/1/2014 0:00:00|40.0729             |-74.7733           |B02512|\n",
      "|max    |4/9/2014 9:59:00|42.1166             |-72.0666           |B02764|\n",
      "+-------+----------------+--------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the descriptive statistics of the data set\n",
    "Uber_DF.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2. Are there companies that dominate in terms of the percentage share of the trips? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#If consider the descriptive statastics above, we can see the total number of trips recorded are 564516\n",
    "#B02682(Schmecken) percentage share can be calculated as (227808/564516)*100 = 40.35 percent\n",
    "#B02598(Hinter LLC) percentage share can be calculated as (183263/564516)*100 = 32.46 percent\n",
    "#B02617(Weiter) percentage share can be calculated as (108001/564516)*100 = 19.13 percent\n",
    "#B02512(Unter LLC) percentage share can be calculated as (35536/564516)*100 = 6.29 percent\n",
    "#B02764(Danach-NY) percentage share can be calculated as (9908/564516)*100 = 1.75 percent\n",
    "#Yes, from above results, it can be observed that B02682(Schmecken) is taking up nearly 41% share of the trips followed by B02598(Hinter LLC) having 33% share, \n",
    "#together resulting in 75% share of the trips can be considered as the dominating companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3.What features (or attributes) are recorded for each Uber trip? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features recorded for each Uber trip are : ['Date/Time', 'Lat', 'Lon', 'Base']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#(Answer):The features recorded for each automobile can be known by the column names in the dataframe\n",
    "UberTrip_Features = Uber_DF.columns\n",
    "print(\"The features recorded for each Uber trip are :\", UberTrip_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Timestamp of pickup(Categorical Type - (String datatype))\n",
    "#2.Latitude of pickup location(Numerical Type - (Double datatype))\n",
    "#3.Longitude of pickup location((Numerical Type - (Double datatype)))\n",
    "#4.TLC base company code affiliated with Uber pickup(Categorical Type - (String datatype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3.Does any attribute require transformation because of data type requirements in Clustering? \n",
    "#If so, identify the feature and comment on the type of transformation required. Include these comments in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#The input columns available to us are,\n",
    "#Timestamp of pickup(String datatype)\n",
    "#Latitude of pickup location(Double datatype)\n",
    "#Longitude of pickup location(Double datatype)\n",
    "#We are not considering a Base label column since it's an unsupervised machine learning technique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case we consider the Timestamp as one of the input column in finding the cluster,\n",
    "#It is better to transform the column before clustering because K-means clustering is a type of unsupervised learning,\n",
    "#which is used when we have unlabeled data (i.e., data without defined categories or groups)  as,\n",
    "#it uses a distance measurement to calculate the similarity between observations\n",
    "#but we need to understand that it does not make a lot of sense to calculate distance between binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number distinct timestamps in our data are: 41999\n"
     ]
    }
   ],
   "source": [
    "#Checking for how many distinct time stamps are there in our data\n",
    "Distinct_Timestamps = Uber_DF.select('Date/Time').distinct().count()\n",
    "print(\"Number distinct timestamps in our data are:\", Distinct_Timestamps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New dataframe for performing transformations\n",
    "Transform_Uber_DF = Uber_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564516 4\n"
     ]
    }
   ],
   "source": [
    "print(Transform_Uber_DF.count(), len(Transform_Uber_DF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to use the column for clustering we need to perform transformation\n",
    "#The Date/Time variable has 41999 different Timestamps in complete dataset of 564516 records,\n",
    "#We need to first transform the variable using a string indexer as it is of string datatype and then,\n",
    "#Then generate dummy variables using one hot encoder to create a significant dummy vector varibles \n",
    "#Because Dummy variables carry good interpretation as it indicates either presence or absence of that attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To assign dummy values to the string variables\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "#For creation of a vector of input variables\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#For assigning the dummy variables\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4.Perform the transformations, if any, identified in step # 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+-------------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|Date/Time_Num|\n",
      "+----------------+-------+--------+------+-------------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512|      30757.0|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|      37044.0|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|      32573.0|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|      36872.0|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|      33766.0|\n",
      "+----------------+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Categorical values into numerical values\n",
    "#StringIndexer arguments = name of input columns and resulting column\n",
    "Timestamp_Indexer = StringIndexer(inputCol = 'Date/Time', outputCol = 'Date/Time_Num').fit(Transform_Uber_DF)\n",
    "#Taking Categorical data and transforming\n",
    "Transform_Uber_DF = Timestamp_Indexer.transform(Transform_Uber_DF)\n",
    "#Checking if numbers are assingned\n",
    "Transform_Uber_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Date/Time_Num|count|\n",
      "+-------------+-----+\n",
      "|26615.0      |9    |\n",
      "|7313.0       |22   |\n",
      "|12493.0      |17   |\n",
      "|13533.0      |16   |\n",
      "|24801.0      |10   |\n",
      "|7487.0       |22   |\n",
      "|20948.0      |12   |\n",
      "|14285.0      |16   |\n",
      "|11935.0      |18   |\n",
      "|27352.0      |8    |\n",
      "|28553.0      |8    |\n",
      "|10924.0      |18   |\n",
      "|3597.0       |28   |\n",
      "|9923.0       |19   |\n",
      "|17884.0      |13   |\n",
      "|1051.0       |36   |\n",
      "|496.0        |41   |\n",
      "|596.0        |40   |\n",
      "|934.0        |37   |\n",
      "|36504.0      |3    |\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To check count of data by Timestamp after creating Date/Time_Num\n",
    "Transform_Uber_DF.groupBy('Date/Time_Num').count().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+-------------+----------------------+\n",
      "|Date/Time       |Lat    |Lon     |Base  |Date/Time_Num|Date/Time_Dummy_Vector|\n",
      "+----------------+-------+--------+------+-------------+----------------------+\n",
      "|4/1/2014 0:11:00|40.769 |-73.9549|B02512|30757.0      |(41998,[30757],[1.0]) |\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|37044.0      |(41998,[37044],[1.0]) |\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|32573.0      |(41998,[32573],[1.0]) |\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|36872.0      |(41998,[36872],[1.0]) |\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|33766.0      |(41998,[33766],[1.0]) |\n",
      "|4/1/2014 0:33:00|40.7383|-74.0403|B02512|33766.0      |(41998,[33766],[1.0]) |\n",
      "|4/1/2014 0:39:00|40.7223|-73.9887|B02512|36372.0      |(41998,[36372],[1.0]) |\n",
      "|4/1/2014 0:45:00|40.762 |-73.979 |B02512|38203.0      |(41998,[38203],[1.0]) |\n",
      "|4/1/2014 0:55:00|40.7524|-73.996 |B02512|34175.0      |(41998,[34175],[1.0]) |\n",
      "|4/1/2014 1:01:00|40.7575|-73.9846|B02512|38886.0      |(41998,[38886],[1.0]) |\n",
      "+----------------+-------+--------+------+-------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check how many distinct values the variables have and assign right number of dummy variables for Date/Time\n",
    "TimeStamp_Encoder = OneHotEncoder(inputCol = 'Date/Time_Num', outputCol = 'Date/Time_Dummy_Vector')\n",
    "Transform_Uber_DF = TimeStamp_Encoder.transform(Transform_Uber_DF)\n",
    "Transform_Uber_DF.show(10,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Date/Time_Dummy_Vector|count|\n",
      "+----------------------+-----+\n",
      "|  (41998,[26832],[1...|    8|\n",
      "|  (41998,[9879],[1.0])|   19|\n",
      "|  (41998,[12365],[1...|   17|\n",
      "|  (41998,[16814],[1...|   14|\n",
      "|  (41998,[6298],[1.0])|   24|\n",
      "|  (41998,[14387],[1...|   16|\n",
      "|  (41998,[6010],[1.0])|   24|\n",
      "|  (41998,[1918],[1.0])|   33|\n",
      "|  (41998,[18803],[1...|   13|\n",
      "|  (41998,[6261],[1.0])|   24|\n",
      "|  (41998,[2429],[1.0])|   31|\n",
      "|  (41998,[2843],[1.0])|   30|\n",
      "|  (41998,[6338],[1.0])|   24|\n",
      "|  (41998,[9105],[1.0])|   20|\n",
      "|  (41998,[25408],[1...|    9|\n",
      "|  (41998,[31616],[1...|    6|\n",
      "|  (41998,[21030],[1...|   11|\n",
      "|  (41998,[26184],[1...|    9|\n",
      "|  (41998,[12685],[1...|   17|\n",
      "|  (41998,[7292],[1.0])|   22|\n",
      "+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To check count of data by Timestamp after creating Date/Time_Dummy_Vector\n",
    "Transform_Uber_DF.groupBy('Date/Time_Dummy_Vector').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4. Perform feature engineering if and where needed, including vectorization of relevant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#As the model to be developed is a set of clusters to group trips by pickup location (longitude and latitude of the pickup location),\n",
    "#we do not consider the Date/Time column for feature engineering\n",
    "#Creating Dataframe for clustering\n",
    "Cluster_DF = Uber_DF.drop('Date/Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|    Lat|     Lon|  Base|\n",
      "+-------+--------+------+\n",
      "| 40.769|-73.9549|B02512|\n",
      "|40.7267|-74.0345|B02512|\n",
      "|40.7316|-73.9873|B02512|\n",
      "|40.7588|-73.9776|B02512|\n",
      "|40.7594|-73.9722|B02512|\n",
      "|40.7383|-74.0403|B02512|\n",
      "|40.7223|-73.9887|B02512|\n",
      "| 40.762| -73.979|B02512|\n",
      "|40.7524| -73.996|B02512|\n",
      "|40.7575|-73.9846|B02512|\n",
      "|40.7256|-73.9869|B02512|\n",
      "|40.7591|-73.9684|B02512|\n",
      "|40.7271|-73.9803|B02512|\n",
      "|40.6463|-73.7896|B02512|\n",
      "|40.7564|-73.9167|B02512|\n",
      "|40.7666|-73.9531|B02512|\n",
      "| 40.758|-73.9761|B02512|\n",
      "|40.7238|-73.9821|B02512|\n",
      "|40.7531|-74.0039|B02512|\n",
      "|40.7389|-74.0393|B02512|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Cluster_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Feature Engineering######\n",
    "#Loading packages for vectorization\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the input columns\n",
    "input_cols=['Lat', 'Lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Assembling\n",
    "vec_assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a transformation\n",
    "Final_Uber_DF = vec_assembler.transform(Cluster_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4.Provide a printout of the schema of your feature-engineered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Lon: double (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Checking schema for feature-engineered data\n",
    "Final_Uber_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question5.To train and then test your model, split the data from UberApril14 into training and test datasets using a 75/25 split.\n",
    "Model_DF = Final_Uber_DF.select(['Base','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Base  |features          |\n",
      "+------+------------------+\n",
      "|B02512|[40.769,-73.9549] |\n",
      "|B02512|[40.7267,-74.0345]|\n",
      "|B02512|[40.7316,-73.9873]|\n",
      "|B02512|[40.7588,-73.9776]|\n",
      "|B02512|[40.7594,-73.9722]|\n",
      "|B02512|[40.7383,-74.0403]|\n",
      "|B02512|[40.7223,-73.9887]|\n",
      "|B02512|[40.762,-73.979]  |\n",
      "|B02512|[40.7524,-73.996] |\n",
      "|B02512|[40.7575,-73.9846]|\n",
      "|B02512|[40.7256,-73.9869]|\n",
      "|B02512|[40.7591,-73.9684]|\n",
      "|B02512|[40.7271,-73.9803]|\n",
      "|B02512|[40.6463,-73.7896]|\n",
      "|B02512|[40.7564,-73.9167]|\n",
      "|B02512|[40.7666,-73.9531]|\n",
      "|B02512|[40.758,-73.9761] |\n",
      "|B02512|[40.7238,-73.9821]|\n",
      "|B02512|[40.7531,-74.0039]|\n",
      "|B02512|[40.7389,-74.0393]|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Model_DF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of data for clustering into required division\n",
    "Train_DF, Test_DF = Model_DF.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423827\n"
     ]
    }
   ],
   "source": [
    "#Question5.Like you did in step 2 above, comment on the percentage distribution of trips among companies in both the training and test datasets.\n",
    "print(Train_DF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TLC base company codes affiliated with Uber pickup in Training Dataset : 5\n"
     ]
    }
   ],
   "source": [
    "#Checking how many distinct companies are there in our data\n",
    "Train_Distinct_TLC_Codes = Train_DF.select('Base').distinct().count()\n",
    "print(\"Number of TLC base company codes affiliated with Uber pickup in Training Dataset :\", Distinct_TLC_Codes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips recorded for each company (by base code) in Training Data :\n",
      "+------+------+\n",
      "|Base  |count |\n",
      "+------+------+\n",
      "|B02682|170957|\n",
      "|B02598|137880|\n",
      "|B02617|80889 |\n",
      "|B02512|26670 |\n",
      "|B02764|7431  |\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(\"Trips recorded for each company (by base code) in Training Data :\")\n",
    "Train_DF.groupBy('Base').count().orderBy('count',ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B02682(Schmecken) is the first company that has highest(170859) distribution of trips\n",
    "#B02598(Hinter LLC) is the second company with high(137578) distribution of trips\n",
    "#B02617(Weiter) is the third company with high(80777) distribution of trips\n",
    "#B02512(Unter LLC) company has less(26668) distribution of trips\n",
    "#B02764(Danach-NY) company has least(7375) distribution of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140689\n"
     ]
    }
   ],
   "source": [
    "print(Test_DF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TLC base company codes affiliated with Uber pickup in Testing Dataset : 5\n"
     ]
    }
   ],
   "source": [
    "#Checking how many distinct companies are there in our data\n",
    "Test_Distinct_TLC_Codes = Test_DF.select('Base').distinct().count()\n",
    "print(\"Number of TLC base company codes affiliated with Uber pickup in Testing Dataset :\", Distinct_TLC_Codes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips recorded for each company (by base code) in Testing Data :\n",
      "+------+-----+\n",
      "|Base  |count|\n",
      "+------+-----+\n",
      "|B02682|56851|\n",
      "|B02598|45383|\n",
      "|B02617|27112|\n",
      "|B02512|8866 |\n",
      "|B02764|2477 |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(\"Trips recorded for each company (by base code) in Testing Data :\")\n",
    "Test_DF.groupBy('Base').count().orderBy('count',ascending = False).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B02682(Schmecken) is the first company that has highest(56949) distribution of trips\n",
    "#B02598(Hinter LLC) is the second company with high(45685) distribution of trips\n",
    "#B02617(Weiter) is the third company with high(27224) distribution of trips\n",
    "#B02512(Unter LLC) company has less(8868) distribution of trips\n",
    "#B02764(Danach-NY) company has least(2511) distribution of trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question5.Are they representative of the overall data? Include your answer as comments in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Yes, they are representative of overall data because the original data has,\n",
    "#If consider the descriptive statastics above, we can see the total number of trips recorded are 564516\n",
    "#B02682(Schmecken) percentage share can be calculated as (227808/564516)*100 = 40.35 percent\n",
    "#B02598(Hinter LLC) percentage share can be calculated as (183263/564516)*100 = 32.46 percent\n",
    "#B02617(Weiter) percentage share can be calculated as (108001/564516)*100 = 19.13 percent\n",
    "#B02512(Unter LLC) percentage share can be calculated as (35536/564516)*100 = 6.29 percent\n",
    "#B02764(Danach-NY) percentage share can be calculated as (9908/564516)*100 = 1.75 percent\n",
    "#Yes, from above results, it can be observed that B02682(Schmecken) is taking up nearly 41% share of the trips followed by B02598(Hinter LLC) having 33% share, \n",
    "#together resulting in 75% share of the trips can be considered as the dominating companies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question6.Build and train KMeans Clustering model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading Kmeans from pyspark\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question6. For this you, will use the Elbow method to identify the number of clusters to start the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=2\n",
      "the sse = 1020.1425193554632\n",
      "------------------------------------------------------------\n",
      "With K=3\n",
      "the sse = 936.6312904709013\n",
      "------------------------------------------------------------\n",
      "With K=4\n",
      "the sse = 630.5608120664851\n",
      "------------------------------------------------------------\n",
      "With K=5\n",
      "the sse = 572.9671476925705\n",
      "------------------------------------------------------------\n",
      "With K=6\n",
      "the sse = 458.50532836857263\n",
      "------------------------------------------------------------\n",
      "With K=7\n",
      "the sse = 308.88755934701015\n",
      "------------------------------------------------------------\n",
      "With K=8\n",
      "the sse = 272.77780005337684\n",
      "------------------------------------------------------------\n",
      "With K=9\n",
      "the sse = 273.8542531570577\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    " #Use a seed value to ensure each iteration starts with the same initial set of conditions. \n",
    "errors = []\n",
    "for k in range(2,10):\n",
    "    #Setting seed value as 1\n",
    "    kmeans = KMeans(featuresCol = 'features',k=k).setSeed(1)\n",
    "    #trainig the model\n",
    "    model = kmeans.fit(Train_DF)\n",
    "    #defining intra cluster distance\n",
    "    intra_cluster_distance = model.computeCost(Train_DF)\n",
    "    #adding theSSes to the list we have taken\n",
    "    errors.append(intra_cluster_distance)\n",
    "    #taking each cluster and printing errors\n",
    "    print(\"With K={}\".format(k))#selection of cluster\n",
    "    #Printing the error\n",
    "    print(\"the sse = \" + str(intra_cluster_distance))\n",
    "    #Seperating the output errors and coefficients by dashes\n",
    "    print(\"---\"*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question6.For this you, will use the Elbow method to identify the number of clusters to start the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use a seed value to ensure each iteration starts with the same initial set of conditions. \n",
    "#Experiment with (n-1) and (n+1) number of clusters, where n is the optimal number found by the Elbow method. \n",
    "#For each run, generate the SSE and Silhouette Coefficient. \n",
    "#Select the best model on the basis of SSE and Silhouette Coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the packages for plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Horizontal axis cluster number\n",
    "#Plotting the cluster\n",
    "cluster_number = range(2,10)\n",
    "#Xlabel\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "#Using scatterplot\n",
    "plt.scatter(cluster_number, errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above we can see Elbow curve occuring at cluster, K=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "#Training the model with K=6 again\n",
    "kmeans = KMeans(featuresCol = 'features',k=6).setSeed(1)\n",
    "    #trainig the model\n",
    "model_6 = kmeans.fit(Train_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Base', 'features', 'prediction']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating the predictions\n",
    "K_6_predictions = model_6.transform(Train_DF)\n",
    "K_6_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         1| 10732|\n",
      "|         3| 15223|\n",
      "|         5|171836|\n",
      "|         4| 44998|\n",
      "|         2|   731|\n",
      "|         0|180307|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seeing the prediction\n",
    "K_6_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|prediction|Base  |count|\n",
      "+----------+------+-----+\n",
      "|1         |B02512|539  |\n",
      "|3         |B02682|6141 |\n",
      "|4         |B02598|15548|\n",
      "|5         |B02682|69578|\n",
      "|2         |B02512|49   |\n",
      "|1         |B02682|4371 |\n",
      "|5         |B02764|2840 |\n",
      "|3         |B02764|235  |\n",
      "|1         |B02617|2174 |\n",
      "|2         |B02598|258  |\n",
      "|1         |B02764|177  |\n",
      "|0         |B02682|72577|\n",
      "|0         |B02617|34961|\n",
      "|0         |B02512|11271|\n",
      "|3         |B02617|3072 |\n",
      "|5         |B02512|12418|\n",
      "|3         |B02512|978  |\n",
      "|2         |B02682|299  |\n",
      "|5         |B02598|55723|\n",
      "|2         |B02764|16   |\n",
      "|3         |B02598|4797 |\n",
      "|4         |B02764|748  |\n",
      "|0         |B02764|3415 |\n",
      "|4         |B02682|17991|\n",
      "|2         |B02617|109  |\n",
      "|0         |B02598|58083|\n",
      "|4         |B02617|9296 |\n",
      "|1         |B02598|3471 |\n",
      "|4         |B02512|1415 |\n",
      "|5         |B02617|31277|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking with TLC codes names\n",
    "K_6_predictions.groupBy('prediction','Base').count().show(40,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient of K = 6 : 0.5077772061022856\n"
     ]
    }
   ],
   "source": [
    "#Clustering evaluation function K=6\n",
    "evaluatorObj = ClusteringEvaluator()\n",
    "#Evaluating the predictions for silhouette coefficient\n",
    "silhouette_coeff_6 = evaluatorObj.evaluate(K_6_predictions)\n",
    "#Printing Silhouette Coefficient\n",
    "print('Silhouette Coefficient of K = 6 : '+ str(silhouette_coeff_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimenting for n-1\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "kmeans = KMeans(featuresCol = 'features',k=5).setSeed(1)\n",
    "    #trainig the model\n",
    "model_5 = kmeans.fit(Train_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Base', 'features', 'prediction']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating the predictions\n",
    "K_5_predictions = model_5.transform(Train_DF)\n",
    "K_5_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         1| 11881|\n",
      "|         3| 45283|\n",
      "|         4|   910|\n",
      "|         2|160513|\n",
      "|         0|205240|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seeing the prediction\n",
    "K_5_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|prediction|Base  |count|\n",
      "+----------+------+-----+\n",
      "|1         |B02512|597  |\n",
      "|3         |B02682|18122|\n",
      "|4         |B02598|312  |\n",
      "|2         |B02512|11743|\n",
      "|1         |B02682|4843 |\n",
      "|3         |B02764|750  |\n",
      "|1         |B02617|2402 |\n",
      "|2         |B02598|51863|\n",
      "|1         |B02764|201  |\n",
      "|0         |B02682|82743|\n",
      "|0         |B02617|39570|\n",
      "|0         |B02512|12841|\n",
      "|3         |B02617|9342 |\n",
      "|3         |B02512|1425 |\n",
      "|2         |B02682|64887|\n",
      "|2         |B02764|2594 |\n",
      "|3         |B02598|15644|\n",
      "|4         |B02764|23   |\n",
      "|0         |B02764|3863 |\n",
      "|4         |B02682|362  |\n",
      "|2         |B02617|29426|\n",
      "|0         |B02598|66223|\n",
      "|4         |B02617|149  |\n",
      "|1         |B02598|3838 |\n",
      "|4         |B02512|64   |\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking with TLC codes names\n",
    "K_5_predictions.groupBy('prediction','Base').count().show(40,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient of K = 5 : 0.5050605796502464\n"
     ]
    }
   ],
   "source": [
    "#Clustering evaluation function K=5\n",
    "evaluatorObj = ClusteringEvaluator()\n",
    "#Evaluating the predictions for silhouette coefficient\n",
    "silhouette_coeff_5 = evaluatorObj.evaluate(K_5_predictions)\n",
    "#Printing Silhouette Coefficient\n",
    "print('Silhouette Coefficient of K = 5 : '+ str(silhouette_coeff_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimenting for n+1\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "#Training the model with K=6 again\n",
    "#Training the model with K=6 again\n",
    "kmeans = KMeans(featuresCol = 'features',k=7).setSeed(1)\n",
    "    #trainig the model\n",
    "model_7 = kmeans.fit(Train_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Base', 'features', 'prediction']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating the predictions\n",
    "K_7_predictions = model_7.transform(Train_DF)\n",
    "K_7_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         1| 10709|\n",
      "|         6|   735|\n",
      "|         3|162372|\n",
      "|         5| 44957|\n",
      "|         4|  3628|\n",
      "|         2| 14671|\n",
      "|         0|186755|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Seeing the prediction\n",
    "K_7_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|prediction|Base  |count|\n",
      "+----------+------+-----+\n",
      "|1         |B02512|538  |\n",
      "|3         |B02682|65771|\n",
      "|4         |B02598|925  |\n",
      "|5         |B02682|17961|\n",
      "|2         |B02512|951  |\n",
      "|1         |B02682|4360 |\n",
      "|6         |B02617|111  |\n",
      "|5         |B02764|746  |\n",
      "|3         |B02764|2659 |\n",
      "|1         |B02617|2165 |\n",
      "|2         |B02598|4603 |\n",
      "|1         |B02764|176  |\n",
      "|6         |B02682|309  |\n",
      "|0         |B02682|75268|\n",
      "|6         |B02764|16   |\n",
      "|0         |B02617|36193|\n",
      "|0         |B02512|11380|\n",
      "|3         |B02617|29461|\n",
      "|5         |B02512|1410 |\n",
      "|3         |B02512|11822|\n",
      "|2         |B02682|5910 |\n",
      "|5         |B02598|15545|\n",
      "|2         |B02764|231  |\n",
      "|6         |B02598|250  |\n",
      "|6         |B02512|49   |\n",
      "|3         |B02598|52659|\n",
      "|4         |B02764|117  |\n",
      "|0         |B02764|3486 |\n",
      "|4         |B02682|1378 |\n",
      "|2         |B02617|2976 |\n",
      "|0         |B02598|60428|\n",
      "|4         |B02617|688  |\n",
      "|1         |B02598|3470 |\n",
      "|4         |B02512|520  |\n",
      "|5         |B02617|9295 |\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking with TLC codes names\n",
    "K_7_predictions.groupBy('prediction','Base').count().show(40,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient of K = 7 : 0.6312335452401061\n"
     ]
    }
   ],
   "source": [
    "#Clustering evaluation function K=7\n",
    "evaluatorObj = ClusteringEvaluator()\n",
    "#Evaluating the predictions for silhouette coefficient\n",
    "silhouette_coeff_7 = evaluatorObj.evaluate(K_7_predictions)\n",
    "#Printing Silhouette Coefficient\n",
    "print('Silhouette Coefficient of K = 7 : '+ str(silhouette_coeff_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question7.Using the best trained model from step 6, test the performance of the model against the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|prediction|Base  |count|\n",
      "+----------+------+-----+\n",
      "|1         |B02512|337  |\n",
      "|3         |B02682|74   |\n",
      "|4         |B02598|21735|\n",
      "|5         |B02682|88   |\n",
      "|2         |B02512|174  |\n",
      "|1         |B02682|2133 |\n",
      "|5         |B02764|3    |\n",
      "|3         |B02764|5    |\n",
      "|1         |B02617|1111 |\n",
      "|2         |B02598|1132 |\n",
      "|1         |B02764|76   |\n",
      "|0         |B02682|25732|\n",
      "|0         |B02617|11563|\n",
      "|0         |B02512|4471 |\n",
      "|3         |B02617|25   |\n",
      "|5         |B02512|19   |\n",
      "|3         |B02512|11   |\n",
      "|2         |B02682|1441 |\n",
      "|5         |B02598|89   |\n",
      "|2         |B02764|49   |\n",
      "|3         |B02598|64   |\n",
      "|4         |B02764|1240 |\n",
      "|0         |B02764|1104 |\n",
      "|4         |B02682|27383|\n",
      "|2         |B02617|703  |\n",
      "|0         |B02598|20662|\n",
      "|4         |B02617|13659|\n",
      "|1         |B02598|1701 |\n",
      "|4         |B02512|3854 |\n",
      "|5         |B02617|51   |\n",
      "+----------+------+-----+\n",
      "\n",
      "the sse = 179.71791133890045\n",
      "Silhouette Coefficient : 0.4465287996232673\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(featuresCol='features',k=6).setSeed(1)\n",
    "model = kmeans.fit(Test_DF)\n",
    "model.transform(Test_DF).groupBy('prediction','Base').count().show(50, truncate = False)\n",
    "SSE = model.computeCost(Test_DF)\n",
    "predictions = model.transform(Test_DF)\n",
    "silhouette_coeff = evaluatorObj.evaluate(predictions)\n",
    "print(\"the sse = \" + str(SSE))\n",
    "print('Silhouette Coefficient : '+ str(silhouette_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model has an accuracy of about 56% as the Silhouette Coefficienthigh value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "#Silhouette coefficients near +1 indicate that the object is far away from the neighboring clusters\n",
    "#But we cant tell it is the best model as our data in both test and original data is unequally distributed by few dominatinting comanies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|B02682|227808|, Most of the highest trips company are in cluster 0,2 for test dataset prediction\n",
    "#|B02598|183263|,Most of the highest trips company are in cluster 0,2 for test dataset prediction\n",
    "#|B02617|108001|,Most of the highest trips company are in cluster 0,2 for test dataset prediction\n",
    "#|B02512| 35536|,2\n",
    "#|B02764|  9908,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>features</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>B02512</td>\n",
       "      <td>[40.5111, -74.3281]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>B02512</td>\n",
       "      <td>[40.5277, -74.3991]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>B02512</td>\n",
       "      <td>[40.5445, -74.155]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>B02512</td>\n",
       "      <td>[40.5613, -74.475]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>B02512</td>\n",
       "      <td>[40.5783, -73.9567]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Base             features  prediction\n",
       "0  B02512  [40.5111, -74.3281]           4\n",
       "1  B02512  [40.5277, -74.3991]           4\n",
       "2  B02512   [40.5445, -74.155]           4\n",
       "3  B02512   [40.5613, -74.475]           4\n",
       "4  B02512  [40.5783, -73.9567]           4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting prdictions into a pandas dataframe(because gives tabulated result)\n",
    "pandas_df = predictions.toPandas()\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8. Do your own research on evaluation metrics, other than the Silhouette Coefficient, that may be used to measure the performance of the KMeans Clustering algorithm as implemented by pyspark’s MLlib in python. You may want to see how your final model performs on these other metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The other excellent metrics for determining the true count of the clusters such as Bayesian Information Criterion (BIC) but it can be applied only if we are willing to extend the clustering algorithm beyond k-means to the more generalized version — Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
